{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Statistics\n",
    "\n",
    "Inferential statistics involves drawing conclusions about a population from a sample. \n",
    "\n",
    "A sample **statistic** is a quantity calculated from the sample values. Typical notation is $\\bar{x}$ for the sample mean, $s$ for the sample standard deviation. The goal of inferential statistics is to estimate values for parameters of the population, such as its mean $\\mu$ and its standard deviation $\\sigma$. \n",
    "\n",
    "Suppose we draw a sample of size $n$. The particular set of values obtained are denoted $x_1, x_2 \\cdots x_n$. These are particular realizations of the random variables $X_1, X_2 \\cdots X_n$. We have a **random sample** if they are independent of each other and identically distributed (iid). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit theorem\n",
    "If $X_1, X_2 \\cdots X_n$ are independent, identically distributed random variables with finite mean and variance $\\mu$ and $\\sigma^2$, and $S_n = \\sum_{i=1}^n X_i$, then the quantity $\\frac{S_n-n\\mu}{\\sigma\\sqrt{n}}$ has a normal distribution asymptotically.\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{S_n-n\\mu}{\\sigma\\sqrt{n}}\\; \\sim \\; N(0,1)\n",
    "\\implies \\lim_{n \\to \\infty} P\\left(a \\le \\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\le b \\right) = \\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-\\frac{u^2}{2}} du\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Let $Z_i = \\frac{X_i-\\mu}{\\sigma\\sqrt(n)},\\quad \\sum Z_i = \\frac{S_n-n\\mu}{\\sigma\\sqrt{n}}$ and note first that\n",
    "$$\n",
    "M_{\\sum Z_i}(t) = E(e^{t\\sum Z_i}) = E(e^{tZ_1}e^{tZ_2} \\cdots e^{tZ_n}) = E(e^{tZ_1})E(e^{tZ_2}) \\cdots E(e^{tZ_n}) = [ M_{Z_1}(t) ]^n\n",
    "$$\n",
    "because the $Z_i$ are independent. Now,\n",
    "$$\n",
    "\\begin{split}\n",
    "M_{Z_1}(t) &= E(e^{tZ_1}) = E\\left(1 + tZ_1  +\\frac{t^2Z_1^2}{2!} + \\frac{t^3Z_1^3}{3!}+\\cdots\\right)\\\\\n",
    "&= 1 + tE(Z_1) + \\frac{t^2E(Z_1^2)}{2!} + \\frac{t^3E(Z_1^3)}{3!}+\\cdots\n",
    "\\end{split}\n",
    "$$\n",
    "But $E(Z_1) = 0, \\; E(Z_1^2) = \\frac{1}{n}, \\; E(Z_1^3)=\\frac{1}{n^{\\frac{3}{2}}}E\\left(\\frac{(X_1-\\mu)^3}{\\sigma^3}\\right)$ and so on. This means that\n",
    "$$\n",
    "\\lim_{n \\to \\infty}[ M_{Z_1}(t) ]^n =  \\lim_{n \\to \\infty} \\left(1+\\frac{1}{n} \\frac{t^2}{2!} + \\frac{1}{n^{\\frac{3}{2}}}E\\left(\\frac{(X_1-\\mu)^3}{\\sigma^3}\\right) + \\cdots \\right)^n\n",
    "$$\n",
    "\n",
    "Here, we make the crucial observation that terms with powers of $n$ in the denominator greater than 1 in this expression go to zero as $n \\to \\infty$. So,\n",
    "$$\n",
    "\\lim_{n \\to \\infty} [ M_{Z_1}(t) ]^n = \\lim_{n \\to \\infty} \\left(1+\\frac{1}{n} \\frac{t^2}{2} \\right)^n = e^{\\frac{t^2}{2}}\n",
    "$$\n",
    "which is the moment generating function of a standard normal distribution. From the uniqueness of moment generating functions, this implies that \n",
    "$$\n",
    "\\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\text{  is distributed normally as  } n \\to \\infty\n",
    "$$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "This is a powerful theorem because it says that if we have enough variables from an arbitrary distribution, their sum will be distributed normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Estimators\n",
    "A function $g(X_1, X_2 \\cdots X_n)$ designed to estimate a population parameter is called an **estimator**. For example, the function\n",
    "$$\n",
    "g(X_1, X_2 \\cdots X_n) = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\n",
    "$$\n",
    "is an estimator for $\\mu$. Note that we could choose some other function as an estimator: a trimmed mean or a median are examples. The value computed from a particular sample is the **estimate**, and is denoted by a hat over the parameter. For example, an estimate of $\\mu$ is $\\hat{\\mu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Bias and Variance of estimators\n",
    "Estimator functions are designed to estimate population parameters. Given a choice among estimators, we can develop criteria to select the best one among them. The main criteria are bias and efficiency. For an estimator function $g$ designed to estimate the population parameter $\\theta$,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{Bias} &\\equiv E(g) - \\theta \\\\\n",
    "\\text{var}(g) &\\equiv E\\left[(g-E(g))^2\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where the expectations are taken over all samples from the population. Clearly, we prefer estimators with low or zero bias and small variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "A metric that incorporates both estimator bias and variance is the mean squared error (MSE). For a given estimator $g(X_1,X_2,\\cdots X_n)$ and population parameter $\\theta$,\n",
    "\n",
    "$$\n",
    "MSE \\equiv E\\left[(g-\\theta)^2\\right] = E\\left[g^2 - 2 \\theta g + \\theta^2\\right] = E(g^2) -2\\theta E(g) + \\theta^2 \n",
    "$$\n",
    "\n",
    "Now, for any random variable $Y, E(Y^2) = \\text{var}(Y) + E(Y)^2$. So,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "MSE &= E(g^2) -2\\theta E(g) + \\theta^2\\\\ \n",
    "&= [E(g)^2 + \\text{var}(g)]-2\\theta E(g)] + \\theta^2\\\\\n",
    "&= [E(g) - \\theta]^2 + \\text{var}(g)\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator for population mean $\\mu$\n",
    "\n",
    "Each of the random variables $X_i$ has an expected value $\\mu$ and variance $\\sigma^2$. We could pick one of them, say $X_1$, as our estimator for the mean. We will then have an unbiased estimator with variance $\\sigma^2$. However, it is possible to pick a better estimator, the sample mean $\\bar{X} = \\frac{1}{n}\\sum X_i$. The sample mean is unbiased, but more importantly, it is more **efficient** because it has a lower standard deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\bar{X}) = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E(X_i) = \\frac{1}{n}\\; n\\mu = \\mu \\end{equation}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{split}\n",
    "\\text{var}(\\bar{X}) &= \\frac{1}{n^2}\\text{var}\\sum X_i = \\frac{1}{n^2} \\sum\\text{var}( X_i) +\\sum_{i<j} 2 \\text{cov}(X_i,X_j)\\\\\n",
    " &= \\frac{1}{n^2} \\sum\\text{var} (X_i) = \\frac{1}{n^2}\\; n\\sigma^2 = \\frac{\\sigma^2}{n}\n",
    "\\end{split}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, we have used the fact that $\\text{cov}(X_i,X_j) = 0\\; \\forall\\; i,j$ since the $\\{X_i\\}$ are all independent of each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator for population variance $\\sigma^2$\n",
    "Let $g(X_1,X_2,\\cdots X_n) = \\sum (X_i - \\bar{X})^2$. Then\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "g &= \\sum (X_i^2 -2\\bar{X}X_i + \\bar{X}^2) = \\sum X_i^2 -2n\\bar{X}^2 + n \\bar{X}^2 = \\sum X_i^2 - n \\bar{X}^2\\\\\n",
    "  \\\\\n",
    "E(g) &= \\sum E(X_i^2) - n E(\\bar{X}^2)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We now use the fact that for any random variable $Y, E(Y^2) = \\text{var}(Y) + E(Y)^2$. So,\n",
    "$$\n",
    "\\begin{split}\n",
    "E(g)&= \\sum(\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n}+\\mu^2\\right)\\\\\n",
    "&= n(\\sigma^2 + \\mu^2) - (\\sigma^2 + n\\mu^2)\\\\\n",
    "&= (n-1)\\sigma^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This implies that the sum of squared deviations from the mean is a biased estimator of the population variance. However, this is easily fixed. The **sample variance**, $s^2$, defined as\n",
    "\\begin{equation}\n",
    "s^2 \\equiv \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar{X})^2\n",
    "\\end{equation}\n",
    "gives us an unbiased estimator of the population variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling distributions\n",
    "\n",
    "As seen above, estimators are random variables whose values vary from sample to sample. The distributions corresponding to estimators are known as **sampling distributions**. Knowing the sampling distribution of an estimator allows us to draw conclusions about population parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the sample mean\n",
    "First, consider the case when the **population is normal**, which implies that $X_i \\sim N(\\mu,\\sigma^2)$ for all $i$. We proceed by considering moment generating functions. For $X \\sim N(\\mu,\\sigma^2)$, the moment generating function $M_{X}(t) \\equiv E\\left(e^{tX}\\right) = e^{\\mu t + \\sigma^2\\frac{t^2}{2}}$. This function in unique.\n",
    "\n",
    "The moment generating function for $\\bar{X}$ is given by\n",
    "\n",
    "$$\n",
    "M_{\\bar{X}}(t) = E\\left(e^{t\\sum\\frac{X_i}{n}}\\right)\n",
    "= E\\left(e^{t\\frac{X_1}{n}}\\dot  e^{t\\frac{X_2}{n}}\\cdots e^{t\\frac{X_n}{n}} \\right)\n",
    "= E\\left(e^{t\\frac{X_1}{n}}\\right)E\\left(e^{t\\frac{X_2}{n}}\\right) \\cdots E\\left(e^{t\\frac{X_n}{n}}\\right)\n",
    "$$\n",
    "\n",
    "where the last equality follows from the fact that the ${X_i}$ are independent of each other.\n",
    "$$\n",
    "\\begin{split}\n",
    "M_{\\bar{X}}(t) &= \\Pi_{i=1}^n M_X(t/n)\\\\\n",
    "&= \\Pi_{i=1}^n e^{\\mu \\frac{t}{n} + \\frac{\\sigma^2}{n^2}\\frac{t^2}{2}}\\\\\n",
    "&= e^{\\sum_{i=1}^n \\mu \\frac{t}{n} + \\frac{\\sigma^2}{n^2}\\frac{t^2}{2}}\\\\\n",
    "&= e^{\\mu t + \\frac{\\sigma^2}{n}\\frac{t^2}{2}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This implies that\n",
    "$$\n",
    "\\boxed{\n",
    "\\bar{X} \\sim N(\\mu,\\frac{\\sigma^2}{n}), \\;\\text{population normal}\n",
    "}\n",
    "$$\n",
    "\n",
    "In case the **population is not normal**, we use the central limit theorem, from which we get\n",
    "$$\n",
    "\\lim_{n\\to\\infty} P\\left(a \\le \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\le b\\right) = \\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-\\frac{u^2}{2}du}\n",
    "$$\n",
    "In other words,\n",
    "$$\n",
    "\\boxed{\n",
    "Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\;\\text{ is normally distributed as } n \\to \\infty \\text{ for a non-normal population}\n",
    "}\n",
    "$$\n",
    "\n",
    "In many cases, the population variance is unknown. In that case we are forced to use a sample estimate of the population variance. To this end, note that\n",
    "$$\n",
    "\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}} = \\frac{\\bar{X}-\\mu}{\\frac{s}{\\sigma}\\;\\frac{\\sigma}{\\sqrt{n}}} = \\frac{(\\bar{X}-\\mu) / (\\sigma/\\sqrt{n})}{\\sqrt{s^2/\\sigma^2}}\n",
    "$$\n",
    "\n",
    "The numerator of the last quantity is a standard normal variable. As shown below, the denominator is a $\\chi^2_{n-1}$ variable divided by its degrees of freedom, $(n-1)$. This implies that the whole quantity has a t distribution with $(n-1)$ degrees of freedom\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}\\; \\text{ is t distributed with } (n-1) \\text{ degrees of freedom, population normal}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the sample variance\n",
    "Assume that the population is normal, with finite mean $\\mu$ and variance $\\sigma^2$. Now,\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sum (X_i-\\mu)^2 &= \\sum\\left( X_i-\\bar{X}+\\bar{X}-\\mu \\right)^2\\\\\n",
    "&= \\sum (X_i-\\bar{X})^2 + \\sum (\\bar{X}-\\mu)^2 + 2 \\sum (X_i-\\bar{X})(\\bar{X}-\\mu)\\\\\n",
    "&= \\sum (X_i-\\bar{X})^2 + n (\\bar{X}-\\mu)^2 + 2 (\\bar{X}-\\mu)\\sum(X_i-\\bar{X})\n",
    "\\end{split}\n",
    "$$\n",
    "Now, since $\\sum(X_i-\\bar{X}) = 0$, we have\n",
    "$$\n",
    "\\sum (X_i-\\mu)^2 = \\sum (X_i-\\bar{X})^2 + n (\\bar{X}-\\mu)^2\n",
    "$$\n",
    "Dividing throughout by $\\sigma^2$,\n",
    "$$\n",
    "\\sum \\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2 = \\sum \\frac{(X_i-\\bar{X})^2}{\\sigma^2} + \\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\right)^2\n",
    "$$\n",
    "We can rewrite this as\n",
    "$$\n",
    "\\sum Z_i^2 = \\frac{(n-1)s^2}{\\sigma^2} + Z_{\\bar{X}}^2\n",
    "$$\n",
    "The $Z_i$ are all standard normal variables, and so is $Z_{\\bar{X}}$, because we are assuming that the population is normal.\n",
    "\n",
    "Let $Q = \\sum Z_i^2, R = \\frac{(n-1)s^2}{\\sigma^2}, S = Z_{\\bar{X}}^2$, and note that\n",
    "$$\n",
    "Q \\sim \\chi_n^2, \\qquad S \\sim \\chi_1^2\n",
    "$$\n",
    "since the sum of $n$ squared standard normal variables is $\\chi_n^2$ distributed.\n",
    "\n",
    "Now, for independent random variables $R, S$ and $Q=R+S$, the moment generating functions can be written as:\n",
    "$$\n",
    "M_Q(t) = E(e^{tQ}) = E(e^{tR}e^{tS}) = E(e^{tR}) E(e^{tS}) = M_R(t) M_S(t) \\implies M_R(t) = \\frac{M_Q(t)}{M_S(t)}\n",
    "$$\n",
    "Since\n",
    "$$\n",
    "M_Q(t) = \\left(1-2t\\right)^{-\\frac{n}{2}} \\text{  and  } M_S(t) = \\left(1-2t\\right)^{-\\frac{1}{2}}\n",
    "$$\n",
    "we get $M_R(t) = \\left(1-2t\\right)^{-\\frac{n-1}{2}}$.\n",
    "\n",
    "Since the moment generating function is unique,\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi_{n-1}^2, \\; \\text{population normal}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of sample proportion\n",
    "Suppose that the population is a Bernoulii distributed random variable $X \\in \\{0,1\\}$ with\n",
    "\n",
    "$$ \n",
    "    P[X=1] = \\pi \\qquad P[X=0] = 1-\\pi \\qquad \\mu = \\pi \\qquad \\sigma^2 = \\pi(1-\\pi) \n",
    "$$\n",
    "    \n",
    "We draw a sample of size $n$, obtaining random variables $X_1, X_2 \\cdots X_n$, and define the sample proportion $p$\n",
    "$$\n",
    "p \\equiv \\bar{X} = \\frac{X_1+X_2+\\cdots+X_n}{n}\n",
    "$$\n",
    "\n",
    "Since the $X_i$ are iid,\n",
    "$$\n",
    "\\begin{split}\n",
    "E(p) &= \\pi \\\\\n",
    "\\sigma^2_p &= \\frac{\\sigma^2}{n} =\\frac{\\pi(1-\\pi)}{n}  \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "Note that $p$ can only take on discrete values $k/n$, where $1\\le k\\le n$. Now, with $k=np_0$,\n",
    "$$\n",
    "P[p=p_0] = P [X_1+X_2 \\cdots X_n = k] = \\binom{n}{k}\\; \\pi^k\\; (1-\\pi)^{n-k}\n",
    "$$\n",
    "which means that the sampling distribution of the sample proportion has a Binomial distribution.\n",
    "\n",
    "Using the Cenrtral Limit Theorem, we know that \n",
    "$$\n",
    "\\lim_{n\\to\\infty} P\\left(a \\le \\frac{\\bar{X}-\\pi}{\\sigma/\\sqrt{n}} \\le b\\right) = \\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-\\frac{u^2}{2}du}\n",
    "$$\n",
    "\n",
    "or, in other words, the random variable\n",
    "\n",
    "$$\n",
    "Z=\\frac{p-\\pi}{\\sqrt{\\frac{\\pi(1-\\pi)}{n}}}\n",
    "$$\n",
    "\n",
    "is asymptotically normal. This is actually a statement that the Binomial distribution is asymptotically normal. Often, textbooks suggest a rule of thumb that this approximation can be used if the conditions $n\\pi \\ge 10$ and $n(1-\\pi) \\ge 10$ are met.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of difference of means\n",
    "\n",
    "Consider two independent normal random variables, $Z_1 \\sim N(\\mu_1,\\sigma^2_1)$ and $Z_2\\sim N(\\mu_2,\\sigma^2_2)$. What is the distribution of $Y = Z_1 - Z_2$? We can answer this with moment generating functions:\n",
    "$$\n",
    "\\begin{split}\n",
    "M_Y(t) &= E(e^{tY}) = E(e^{t(Z_1-Z_2)}) = E(e^{tZ_1}\\;e^{-tZ_2}) = E(e^{tZ_1})E(e^{-tZ_2})\\\\\n",
    "&=e^{\\mu_1 t + \\sigma_1^2 t^2/2}\\; e^{-\\mu_2 t + \\sigma_2^2 t^2/2}\\\\\n",
    "&= e^{(\\mu_1-\\mu_2) t + (\\sigma_1^2+\\sigma_2^2) t^2/2}\n",
    "\\end{split}\n",
    "$$\n",
    "which means that $Y \\sim N(\\mu_1-\\mu_2, \\sigma_1^2+\\sigma_2^2)$.\n",
    "\n",
    "Now consider the situation when we have two independent samples of sizes $n$ and $m$ respectively, from normally distributed populations. Given their sample means $\\bar{X}$ and $\\bar{Y}$, what can we say about the difference of the population means?\n",
    "\n",
    "Since the populations are normal, we know that $\\bar{X} \\sim N(\\mu_X,\\frac{\\sigma_X^2}{n})$ and $\\bar{Y} \\sim N(\\mu_Y,\\frac{\\sigma_Y^2}{m})$. So,\n",
    "\n",
    "$$\n",
    "\\bar{X}-\\bar{Y} \\sim N\\left(\\mu_X-\\mu_Y, \\frac{\\sigma_X^2}{n}+\\frac{\\sigma_Y^2}{m}\\right)\n",
    "$$\n",
    "\n",
    "We often do not know the population variances, and in analogy with the one sample case, we would like to be able to set up a statistic using sample variances that is t distributed. This canbe done if we assume $\\sigma_X = \\sigma_Y = \\sigma$. Under this assumption\n",
    "$$\n",
    "Z = \\frac{\\bar{X}-\\bar{Y} -(\\mu_X-\\mu_Y)}{\\sigma \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim N(0,1)\n",
    "$$\n",
    "Given that $T = Z/\\sqrt{V/k}$ is t distributed if $Z \\sim N(0,1)$ and $V \\sim \\chi^2_k$, we seek a $\\chi^2$ distributed estimator of the variance of the sampling distribution of the difference of means. For this, we note that\n",
    "$$\n",
    "\\frac{(n-1) s_X^2}{\\sigma^2} \\sim \\chi^2_{n-1}, \\quad \\frac{(m-1) s_Y^2}{\\sigma^2} \\sim \\chi^2_{m-1},\\quad \\frac{(n-1)s^2_X + (m-1)s^2_Y}{\\sigma^2} \\sim \\chi^2_{n+m-2}\n",
    "$$\n",
    "So we define a **pooled variance** estimator\n",
    "$$\n",
    "s_p \\equiv \\frac{(n-1)s^2_X + (m-1)s^2_Y}{n+m-2} \\implies V = \\frac{(n+m-2) s^2_p}{\\sigma^2} \\sim \\chi^2_{n+m-2}\n",
    "$$\n",
    "This means that\n",
    "$$\n",
    "\\frac{Z}{\\sqrt{V/(n+m-2)}} = \\frac{Z}{s_p/\\sigma} = \\frac{\\bar{X}-\\bar{Y} -(\\mu_X-\\mu_Y)}{s_p \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim T_{(n+m-2)}\n",
    "$$\n",
    "Summarizing,\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{With }\\; s_p \\equiv \\frac{(n-1)s^2_X + (m-1)s^2_Y}{n+m-2}, \\qquad \\frac{\\bar{X}-\\bar{Y} -(\\mu_X-\\mu_Y)}{s_p \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim T_{(n+m-2)}, \\quad \\text{populations normal, pooled variance}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval estimation\n",
    "\n",
    "Sample statistics are point estimates of population parameters, since they give us one number as an estimate. If we know the sampling distribution of the statistic, we can calculate a range within which the population parameter falls with a pre-specified probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population mean\n",
    "Suppose that we have a sample mean and we seek a range for the population mean with 95% probability. Since the population is normal, we know that the sampling distribution of the mean will be normal. This implies that\n",
    "$$\n",
    "P \\left(-z_{0.025} \\le \\frac{\\bar{X}-\\mu}{\\sigma\\sqrt{n}} \\le z_{0.025} \\right) = 0.95\n",
    "$$\n",
    "or, in general,\n",
    "$$\n",
    "P \\left(-z_{\\alpha/2} \\le \\frac{\\bar{X}-\\mu}{\\sigma\\sqrt{n}} \\le z_{\\alpha/2} \\right) = (1-\\alpha)\n",
    "$$\n",
    "Here $\\alpha$ is known as the **level of significance** and $(1-\\alpha)$ is called the **confidence level**. \n",
    "$$\n",
    "\n",
    "The **confidence interval** for the population mean results from inverting the inequalities\n",
    "\n",
    "$$\n",
    "\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\quad \\text{with confidence level } (1-\\alpha), \\text{ for a normal population}\n",
    "$$\n",
    "\n",
    "As is generally the case, if the population variance is unknown, we use the sample variance and critical values for the t distribution with $(n-1)$ degrees of freedom.\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\bar{X}-t_c\\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\bar{X}+t_c\\frac{\\sigma}{\\sqrt{n}}\\quad \\text{with confidence level } (1-\\alpha), \\text{ for a normal population}\n",
    "}\n",
    "$$\n",
    "\n",
    "The critical value $t_c$ is given by\n",
    "$$\n",
    "P(T_{n-1} \\le t_c) = 1-\\frac{\\alpha}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4948364398216353"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.2621571627409915"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-0.11029681585827622, 1.0999696955015468)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interval estimation of population mean assuming normally distributed population\n",
    "\n",
    "np.random.seed(12345)\n",
    "n=10\n",
    "v1 = stats.norm.rvs(size=n)  # Standard normal random variates\n",
    "\n",
    "xbar = np.mean(v1)\n",
    "s = np.std(v1, ddof=1)    # ddof=1 uses (n-1) for the denominator, so gives us sample sd\n",
    "xbar\n",
    "\n",
    "ConfLevel = 0.95\n",
    "alpha = 1 - ConfLevel     # Level of significance\n",
    "\n",
    "tc = stats.t.ppf(1-alpha/2,n-1) # critical value; P(-\\infty,tc) = prob in right tail = 1-alpha/2\n",
    "tc\n",
    "xbar-tc*s/np.sqrt(n),xbar+tc*s/np.sqrt(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population variance\n",
    "\n",
    "For a normal population, $\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}$. The $\\chi^2$ density is not symmetric and so the left and right critical values for a given confidence level are arbitrary, to some extent. One way to pick specific critical values is to search for the values that lead to the smallest interval, but the convention is just to pick the values that lead to the same remaining probability on either side of the range.\n",
    "$$\n",
    "P\\left(c_1 \\le \\frac{(n-1)s^2}{\\sigma^2} \\le c_2 \\right) = 1-\\alpha\n",
    "$$\n",
    "such that\n",
    "$$\n",
    "P(\\chi^2_{n-1} \\le c_1) = \\frac{\\alpha}{2} \\quad P(\\chi^2_{n-1} \\le c_2) = 1 - \\frac{\\alpha}{2}\n",
    "$$\n",
    "This yields\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{(n-1) s^2}{c_2} \\le \\sigma^2 \\le \\frac{(n-1) s^2}{c_1}\\quad \\text{with confidence level } (1-\\alpha), \\text{ for a normal population}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample variance:  0.8459179615720821\n",
      "Critical values:  2.7003894999803584 19.02276779864163\n",
      "Probabilities up to critical values:  0.025000000000000015 0.975\n",
      "95.0 % confidence interval for the variance:  0.3385519314309392 2.3849132798950827\n"
     ]
    }
   ],
   "source": [
    "# Interval estimation of population variance assuming normally distributed population\n",
    "\n",
    "np.random.seed(12345)\n",
    "n=10\n",
    "v1 = stats.norm.rvs(size=n)  # Standard normal random variates\n",
    "s = np.std(v1, ddof=1)    # ddof=1 uses (n-1) for the denominator, so gives us sample sd\n",
    "print(\"Sample variance: \",s)\n",
    "\n",
    "ConfLevel = 0.95\n",
    "alpha = 1 - ConfLevel     # Level of significance\n",
    "\n",
    "c1 = stats.chi2.ppf(alpha/2,n-1)    # P(0,c1) = prob in left tail = alpha/2\n",
    "c2 = stats.chi2.ppf(1-alpha/2,n-1)  # P(0,c2) = 1-prob in right tail = 1-alpha/2\n",
    "\n",
    "print(\"Critical values: \", c1,c2)\n",
    "print(\"Probabilities up to critical values: \",stats.chi2.cdf(c1,n-1), stats.chi2.cdf(c2,n-1))\n",
    "\n",
    "# Interval estimate\n",
    "print(100*ConfLevel,\"% confidence interval for the variance: \", (n-1)*(s**2)/c2,(n-1)*(s**2)/c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One sample t-test\n",
    "\n",
    "With a sample of size $n$, mean $\\bar{x}$, standard deviation $s$, and population mean $\\mu$, the test statistic \n",
    "$$\n",
    "t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\n",
    "$$\n",
    "has the t distribution with $n-1$ degrees of freedom.\n",
    "\n",
    "In hypothesis testing, we normally compute the p-value, which is the probability of occurrence of values more extreme than the computed test statistic. This depends on the form of the alternative hypothesis.\n",
    "$$\n",
    "\\begin{split}\n",
    "H_0 &: \\mu = 0 \\\\\n",
    "H_a &: \\mu \\ne 0 \\\\\n",
    "p &= P[T \\le -|t|] + P[T \\ge |t|] \\hspace{1in}\\text{Two-tailed t test}\\\\\n",
    "\\\\\n",
    "H_0 &: \\mu = 0 \\\\\n",
    "H_a &: \\mu \\lt 0 \\\\\n",
    "p &= P[T \\le -|t|] \\hspace{1in}\\text{Lower-tailed t test}\\\\\n",
    "\\\\\n",
    "H_0 &: \\mu = 0 \\\\\n",
    "H_a &: \\mu \\gt 0 \\\\\n",
    "p &= P[T \\ge |t|] \\hspace{1in}\\text{Upper-tailed t test}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8498368520005377, 0.09737624227318431)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=1.8498368520005373, pvalue=0.09737624227318435)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo of scipy.stats one sample t test function\n",
    "\n",
    "np.random.seed(12345)\n",
    "n=10\n",
    "v1 = stats.norm.rvs(size=n)  # Standard normal random variates\n",
    "\n",
    "xbar = np.mean(v1)\n",
    "s = np.std(v1, ddof=1)    # ddof=1 uses (n-1) for the denominator, so gives us sample sd\n",
    "\n",
    "# Default behavior of ttest_1samp:\n",
    "# H0: mu = 0, Ha: mu not equal to zero\n",
    "\n",
    "t =  xbar/ (s/np.sqrt(n))    \n",
    "p = 2*(1-stats.t.cdf(t,n-1)) # Two-sided test, so twice the tail probability. Note that degrees of freedom = n-1\n",
    "t,p\n",
    "stats.ttest_1samp(v1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, -2.23606797749979, 0.013567791664809134)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = 43\n",
    "xbar = 40\n",
    "n = 125\n",
    "s = 15\n",
    "t =  (xbar-mu)/ (s/np.sqrt(n))\n",
    "p = stats.t.cdf(t,n-1)\n",
    "n, t,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04992258704151217"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-1.6572349701441826"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-1.9728699462074992"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.t.cdf(-1.658,124)\n",
    "stats.t.ppf(0.05,124)    # Percent Point Function (inverse of cdf)\n",
    "\n",
    "stats.t.ppf(0.025,185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.452984716810821"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3822396/3)/(96092902/185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6534284283390934"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.f.ppf(0.95,3,185)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
